{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f311dc2f-aa1b-4294-b6f2-a2fc047ca310",
   "metadata": {},
   "source": [
    "pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a33498a-927e-4a49-8549-18e94c4421aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_19004\\3582711916.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\transformers\\quantizers\\auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Device set to use cuda:0\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_19004\\3582711916.py:34: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "# ========== 1. 加载 FAISS 向量库 ==========\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"qa_index_cleaned\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ========== 2. 加载量化 Qwen 模型 ==========\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ========== 3. 构建 RAG Chain ==========\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# 全局聊天记录\n",
    "chat_history = []\n",
    "\n",
    "# ========== 4. 定义问答函数 ==========\n",
    "def rag_answer(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    sources = [doc.page_content[:300] + (\"...\" if len(doc.page_content) > 300 else \"\") for doc in source_docs]\n",
    "    return answer, sources\n",
    "\n",
    "# ========== 5. 生成HTML聊天内容 ==========\n",
    "def chatbot_fn(user_input):\n",
    "    global chat_history\n",
    "    answer, sources = rag_answer(user_input)\n",
    "    chat_history.append((user_input, answer, sources))\n",
    "\n",
    "    user_avatar = \"https://i.imgur.com/9neE8kD.png\"\n",
    "    bot_avatar = \"https://i.imgur.com/62HqZa9.png\"\n",
    "\n",
    "    html = \"\"\n",
    "    for user_text, bot_text, sources in chat_history:\n",
    "        sources_html = \"\"\n",
    "        if sources:\n",
    "            sources_items = \"\".join([f\"<li style='margin-bottom:6px;'>{src}</li>\" for src in sources])\n",
    "            sources_html = f\"\"\"\n",
    "            <details style=\"margin-top:10px;\">\n",
    "                <summary style=\"cursor:pointer; font-weight:bold;\">📄 查看来源文档 ({len(sources)})</summary>\n",
    "                <ul style=\"padding-left:20px; margin-top:8px; color:#555;\">{sources_items}</ul>\n",
    "            </details>\n",
    "            \"\"\"\n",
    "        html += f\"\"\"\n",
    "        <div style=\"display:flex; align-items:flex-start; margin-bottom:10px;\">\n",
    "          <img src=\"{user_avatar}\" style=\"width:40px; height:40px; border-radius:50%; margin-right:10px;\"/>\n",
    "          <div style=\"background:#DCF8C6; padding:10px; border-radius:10px; max-width:70%; white-space:pre-wrap;\">{user_text}</div>\n",
    "        </div>\n",
    "        <div style=\"display:flex; align-items:flex-start; justify-content:flex-end; margin-bottom:20px;\">\n",
    "          <div style=\"background:#F1F0F0; padding:10px; border-radius:10px; max-width:70%; white-space:pre-wrap;\">{bot_text}{sources_html}</div>\n",
    "          <img src=\"{bot_avatar}\" style=\"width:40px; height:40px; border-radius:50%; margin-left:10px;\"/>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    return html\n",
    "\n",
    "# ========== 6. 清空聊天历史 ==========\n",
    "def clear_chat():\n",
    "    global chat_history\n",
    "    chat_history = []\n",
    "    return \"\"\n",
    "\n",
    "# ========== 7. 启动 Gradio UI ==========\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ChatGPT 风格 RAG 问答助手\")\n",
    "\n",
    "    chat_display = gr.HTML()\n",
    "    msg = gr.Textbox(placeholder=\"请输入问题，回车发送...\", label=\"输入\")\n",
    "    clear = gr.Button(\"清空对话\")\n",
    "\n",
    "    msg.submit(chatbot_fn, inputs=msg, outputs=chat_display)\n",
    "    clear.click(clear_chat, outputs=chat_display)\n",
    "\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port=7865)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62115fb3-4a91-4f23-8016-2dafe1ede5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
