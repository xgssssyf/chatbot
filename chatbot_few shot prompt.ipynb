{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307b0b4-445f-47d4-bfa1-c3c26a5f60e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import torch\n",
    "\n",
    "def load_model():\n",
    "    model_id = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "    # unsloth/qwen2-1.5b-bnb-4bit\n",
    "    # unsloth/DeepSeek-R1-Distill-Qwen-1.5B-bnb-4bit\n",
    "    max_seq_length = 2048\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_id,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=torch.float16,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "\n",
    "def build_prompt():\n",
    "    template = \"\"\"\n",
    "You are a professional career advice AI chatbot, specializing in helping undergraduate and postgraduate students with their career questions.  \n",
    "Provide accurate, encouraging, and practical advice based on the student’s question.\n",
    "\n",
    "Here are some examples of how you should respond:\n",
    "\n",
    "Student: I’m about to graduate and don’t know how to write a resume. Can you help?  \n",
    "Chatbot: Of course! A good resume highlights your education, internships, and skills clearly. Keep it concise—usually one page is best. I can suggest some resume templates or connect you with a career advisor for more personalized help.\n",
    "\n",
    "Student: I’m not sure what career suits me. Any advice?  \n",
    "Chatbot: That’s a common concern. You might want to try some career interest assessments to learn more about your strengths and preferences. Attending career talks and workshops can also help. If you want, I can help you book a session with a career counselor for deeper guidance.\n",
    "\n",
    "Student: Where can I find internship opportunities?  \n",
    "Chatbot: Great question! Popular internship websites include LinkedIn, Indeed, and your university’s career portal. Also, consider attending job fairs and networking with alumni. I can help you create an internship application plan if you'd like.\n",
    "\n",
    "Student: I feel anxious about my future and don’t know what to do.  \n",
    "Chatbot: It’s completely normal to feel this way. Try some relaxation techniques and talk to friends or mentors about your concerns. If you need more support, I recommend reaching out to your university’s counseling or career services.\n",
    "\n",
    "Student: Can you tell me about the hiring process at a specific company?  \n",
    "Chatbot: Detailed info about specific company hiring processes can be complex. I suggest contacting your university’s career center or your tutor for personalized advice. Meanwhile, I can help find publicly available info if you want.\n",
    "\n",
    "Now, please answer the following question accordingly:\n",
    "\n",
    "Student: {question}\n",
    "Chatbot:\"\"\"\n",
    "    return PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def create_chain(llm, prompt_template):\n",
    "    return LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\" Loading model, please wait...\")\n",
    "    llm = load_model()\n",
    "\n",
    "    print(\" Model loaded. Building chat chain...\")\n",
    "    prompt_template = build_prompt()\n",
    "    qa_chain = create_chain(llm, prompt_template)\n",
    "\n",
    "    print(\" Career Advice Chatbot is ready! Enter your questions (type 'exit' to quit).\")\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"\\nYou: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\" Goodbye! Wish you success in your career!\")\n",
    "            break\n",
    "\n",
    "        # Use invoke() to get output only, avoid printing the whole prompt\n",
    "        raw = qa_chain.invoke({\"question\": user_input})[\"text\"]\n",
    "        answer = raw.split(\"Chatbot:\")[-1].strip()\n",
    "        print(f\"\\nAI: {answer}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebce2d5-9e8c-4656-be3f-cf9fe99a187c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "import faiss\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Load test text data\n",
    "with open(\"test_texts.pkl\", \"rb\") as f:\n",
    "    test_texts = pickle.load(f)\n",
    "\n",
    "# Initialize embedding model (same as used in QA)\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load FAISS index and original text library (for retrieval context)\n",
    "index = faiss.read_index(\"qa_index.faiss\")\n",
    "with open(\"qa_chunks.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "# Load local Qwen2 model (4bit)\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "max_seq_length = 2048\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def answer_question(query, top_k=3):\n",
    "    # 4.1 Encode query\n",
    "    query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # 4.2 Search top_k relevant chunks\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    \n",
    "    # 4.3 Get retrieved texts\n",
    "    retrieved_chunks = [texts[i] for i in I[0]]\n",
    "    \n",
    "    # 4.4 Join context\n",
    "    context = \"\\n---\\n\".join(retrieved_chunks)\n",
    "    \n",
    "    # 4.5 Construct prompt in English\n",
    "    prompt = f\"\"\"You are an intelligent QA assistant. Please answer the user's question based on the following background knowledge:\n",
    "Background documents:\n",
    "{context}\n",
    "Here are some examples of how you should respond:\n",
    "Student: I'm about to graduate and don't know how to write a resume. Can you help?  \n",
    "Chatbot: Of course! A good resume highlights your education, internships, and skills clearly. Keep it concise—usually one page is best. I can suggest some resume templates or connect you with a career advisor for more personalized help.\n",
    "\n",
    "Student: I'm not sure what career suits me. Any advice?  \n",
    "Chatbot: That's a common concern. You might want to try some career interest assessments to learn more about your strengths and preferences. Attending career talks and workshops can also help. If you want, I can help you book a session with a career counselor for deeper guidance.\n",
    "\n",
    "Student: I feel anxious about my future and don't know what to do.  \n",
    "Chatbot: It's completely normal to feel this way. Try some relaxation techniques and talk to friends or mentors about your concerns. If you need more support, I recommend reaching out to your university's counseling or career services.\n",
    "\n",
    "Student: Can you tell me about the hiring process at a specific company?  \n",
    "Chatbot: Detailed info about specific company hiring processes can be complex. I suggest contacting your university's career center or your tutor for personalized advice. Meanwhile, I can help find publicly available info if you want.\n",
    "\n",
    "Student: Where can I find internship opportunities?  \n",
    "Chatbot: Great question! Popular internship websites include LinkedIn, Indeed, and your university's career portal. Also, consider attending job fairs and networking with alumni. I can help you create an internship application plan if you'd like.\n",
    "\n",
    "Now, please answer the following question accordingly:\n",
    "Student: {query}\n",
    "Chatbot:\"\"\"\n",
    "    # 4.6 Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 4.7 Generate answer with sampling for diversity\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # 4.8 Decode output\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # 4.9 Extract answer only\n",
    "    answer = generated_text[len(prompt):].strip()\n",
    "    return answer\n",
    "\n",
    "# Extract questions and reference answers from Q&A text chunks (supports multi-line answers)\n",
    "def extract_qa(text):\n",
    "    q_match = re.search(r\"Q:\\s*(.*)\", text)\n",
    "    a_match = re.search(r\"A:\\s*(.*)\", text, re.DOTALL)\n",
    "    question = q_match.group(1).strip() if q_match else \"\"\n",
    "    answer = a_match.group(1).strip() if a_match else \"\"\n",
    "    return question, answer\n",
    "\n",
    "def evaluate_semantic_similarity(test_texts, max_samples=200):\n",
    "    \"\"\"\n",
    "    Calculate semantic similarity evaluation\n",
    "    Returns: average similarity, similarity list, detailed results\n",
    "    \"\"\"\n",
    "    total = min(len(test_texts), max_samples)\n",
    "    similarities = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    print(f\" Starting semantic similarity evaluation (Total {total} samples)...\")\n",
    "    \n",
    "    for i, text in enumerate(test_texts[:total]):\n",
    "        question, true_answer = extract_qa(text)\n",
    "        if not question or not true_answer:\n",
    "            print(f\"Skipping sample {i+1}, missing question or answer.\")\n",
    "            continue\n",
    "        \n",
    "        # Generate model answer\n",
    "        pred_answer = answer_question(question)\n",
    "        if not pred_answer:\n",
    "            print(f\"Warning: Empty prediction for query: {question}\")\n",
    "            pred_answer = \"\"\n",
    "        \n",
    "        # Calculate semantic similarity\n",
    "        try:\n",
    "            true_vec = embedder.encode([true_answer], convert_to_numpy=True)\n",
    "            pred_vec = embedder.encode([pred_answer], convert_to_numpy=True)\n",
    "            sim = cosine_similarity(true_vec, pred_vec)[0][0]\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity for sample {i+1}: {e}\")\n",
    "            sim = 0.0\n",
    "        \n",
    "        similarities.append(sim)\n",
    "        \n",
    "        # Save detailed results\n",
    "        detailed_results.append({\n",
    "            \"index\": i + 1,\n",
    "            \"question\": question,\n",
    "            \"reference_answer\": true_answer,\n",
    "            \"generated_answer\": pred_answer,\n",
    "            \"similarity_score\": float(sim)\n",
    "        })\n",
    "        \n",
    "        print(f\"Sample {i+1}/{total}\")\n",
    "        print(f\"Question: {question[:80]}...\")\n",
    "        print(f\"Reference answer: {true_answer[:80]}...\")\n",
    "        print(f\"Generated answer: {pred_answer[:80]}...\")\n",
    "        print(f\"Similarity: {sim:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Calculate statistical metrics\n",
    "    similarities = np.array(similarities)\n",
    "    \n",
    "    results = {\n",
    "        \"total_questions\": len(similarities),\n",
    "        \"mean_similarity\": float(np.mean(similarities)),\n",
    "        \"std_similarity\": float(np.std(similarities)),\n",
    "        \"median_similarity\": float(np.median(similarities)),\n",
    "        \"min_similarity\": float(np.min(similarities)),\n",
    "        \"max_similarity\": float(np.max(similarities)),\n",
    "        \"detailed_results\": detailed_results\n",
    "    }\n",
    "    \n",
    "    return results, similarities\n",
    "\n",
    "def print_evaluation_summary(results, similarities):\n",
    "    \"\"\"Print evaluation results summary\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Semantic Similarity Evaluation Results\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total test samples: {results['total_questions']}\")\n",
    "    print(f\"Mean semantic similarity: {results['mean_similarity']:.4f}\")\n",
    "    print(f\"Standard deviation: {results['std_similarity']:.4f}\")\n",
    "    print(f\"Median: {results['median_similarity']:.4f}\")\n",
    "    print(f\"Minimum: {results['min_similarity']:.4f}\")\n",
    "    print(f\"Maximum: {results['max_similarity']:.4f}\")\n",
    "    \n",
    "    # Similarity distribution\n",
    "    print(f\"\\n Similarity Distribution:\")\n",
    "    print(f\"[0.8-1.0] (Excellent): {np.sum(similarities >= 0.8)} ({np.mean(similarities >= 0.8)*100:.1f}%)\")\n",
    "    print(f\"[0.6-0.8) (Good): {np.sum((similarities >= 0.6) & (similarities < 0.8))} ({np.mean((similarities >= 0.6) & (similarities < 0.8))*100:.1f}%)\")\n",
    "    print(f\"[0.4-0.6) (Fair): {np.sum((similarities >= 0.4) & (similarities < 0.6))} ({np.mean((similarities >= 0.4) & (similarities < 0.6))*100:.1f}%)\")\n",
    "    print(f\"[0.0-0.4) (Poor): {np.sum(similarities < 0.4)} ({np.mean(similarities < 0.4)*100:.1f}%)\")\n",
    "\n",
    "def show_examples(results, num_examples=3):\n",
    "    \"\"\"Show best and worst examples\"\"\"\n",
    "    detailed = results[\"detailed_results\"]\n",
    "    sorted_results = sorted(detailed, key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "    \n",
    "    print(f\"\\n Top {num_examples} highest similarity samples:\")\n",
    "    for i, item in enumerate(sorted_results[:num_examples]):\n",
    "        print(f\"\\n--- Sample {i+1} (Similarity: {item['similarity_score']:.4f}) ---\")\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"Reference answer: {item['reference_answer'][:100]}...\")\n",
    "        print(f\"Generated answer: {item['generated_answer'][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n Bottom {num_examples} lowest similarity samples:\")\n",
    "    for i, item in enumerate(sorted_results[-num_examples:]):\n",
    "        print(f\"\\n--- Sample {i+1} (Similarity: {item['similarity_score']:.4f}) ---\")\n",
    "        print(f\"Question: {item['question']}\")\n",
    "        print(f\"Reference answer: {item['reference_answer'][:100]}...\")\n",
    "        print(f\"Generated answer: {item['generated_answer'][:100]}...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run semantic similarity evaluation\n",
    "    results, similarities = evaluate_semantic_similarity(test_texts, max_samples=200)\n",
    "    \n",
    "    # Print statistical summary\n",
    "    print_evaluation_summary(results, similarities)\n",
    "    \n",
    "    # Show examples\n",
    "    show_examples(results)\n",
    "    \n",
    "    # Save detailed results to file\n",
    "    with open(\"semantic_similarity_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\n Detailed results saved to 'semantic_similarity_results.json'\")\n",
    "    print(f\" Evaluation completed! Mean semantic similarity: {results['mean_similarity']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
