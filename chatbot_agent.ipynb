{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f94d0-45bd-464f-810b-f511a6816f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool\n",
    "\n",
    "# ======== 1. Load embeddings model and vector store ========\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"qa_index_cleaned\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ======== 2. Load local Qwen2-1.5B quantized model (4bit) ========\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ======== 3. Build RAG QA tool ========\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"rag_qa_tool\",\n",
    "    func=qa_chain.run,\n",
    "    description=\"Use this tool only to answer questions based on the internal knowledge base. It does NOT have functions like 'learn' or others.\"\n",
    ")\n",
    "\n",
    "# ======== 4. Build LangGraph Agent ========\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from typing import TypedDict, List, Union\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Wrap as LangGraph-compatible runnable\n",
    "def simple_react_agent_runnable(state):\n",
    "    input_text = state[\"input\"]\n",
    "    steps = state.get(\"steps\", [])\n",
    "    output = agent_chain.run(input_text)\n",
    "    return {\"input\": input_text, \"steps\": steps, \"final\": output}\n",
    "\n",
    "class ToolExecutor:\n",
    "    def __init__(self, tools):\n",
    "        self.tool_map = {tool.name: tool.func for tool in tools}\n",
    "\n",
    "    def invoke(self, tool_input):\n",
    "        # Only one tool in this setup\n",
    "        return self.tool_map[\"rag_qa_tool\"](tool_input)\n",
    "\n",
    "# Define agent state structure\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    steps: List[tuple[AgentAction, str]]\n",
    "    final: Union[str, None]\n",
    "\n",
    "tools = [rag_tool]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# Node function: call agent\n",
    "def call_agent(state: AgentState) -> AgentState:\n",
    "    return simple_react_agent_runnable(state)\n",
    "\n",
    "# Node function: call tool\n",
    "def call_tool(state: AgentState) -> AgentState:\n",
    "    output = tool_executor.invoke(state[\"input\"])\n",
    "    return {\"input\": state[\"input\"], \"steps\": state.get(\"steps\", []), \"final\": output}\n",
    "\n",
    "# Control flow: decide whether to continue reasoning\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    return \"tools\" if state[\"final\"] is None else END\n",
    "\n",
    "# Build LangGraph graph structure\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_agent)\n",
    "graph.add_node(\"tools\", call_tool)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue, {\n",
    "    \"tools\": \"tools\",\n",
    "    END: END\n",
    "})\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# Compile graph (with memory caching)\n",
    "runnable = graph.compile()\n",
    "\n",
    "# LangGraph agent run function\n",
    "def run_langgraph_agent(query: str):\n",
    "    inputs = {\"input\": query, \"steps\": [], \"final\": None}\n",
    "    result = runnable.invoke(inputs)\n",
    "    return result[\"final\"]\n",
    "\n",
    "# ======== 5. CLI Interaction ========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"LangGraph RAG Agent started. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"Enter your question:\\n> \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        result = run_langgraph_agent(query)\n",
    "        print(f\"\\nAnswer: {result}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fb4f5-e65f-40da-9df3-5d170e428626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from typing import TypedDict, List, Union\n",
    "\n",
    "# ======== 1. Load embeddings model and vector store ========\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"qa_index_cleaned\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# ======== 2. Load local Qwen2-1.5B quantized model (4bit) ========\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# ======== 3. Build RAG QA tool ========\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"rag_qa_tool\",\n",
    "    func=qa_chain.run,\n",
    "    description=\"Use this tool only to answer questions based on the internal knowledge base. It does NOT have any functions like 'learn' or others.\"\n",
    ")\n",
    "\n",
    "tools = [rag_tool]\n",
    "\n",
    "# ======== 4. Initialize LangChain Agent ========\n",
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "# ======== 5. Define Agent state structure ========\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    steps: List[tuple]\n",
    "    final: Union[str, None]\n",
    "\n",
    "# ======== 6. Agent call function ========\n",
    "def call_agent(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Calls the LangChain agent with input text.\n",
    "    Returns the output in 'final'.\n",
    "    \"\"\"\n",
    "    input_text = state[\"input\"]\n",
    "    try:\n",
    "        output = agent_chain.run(input_text)\n",
    "    except Exception as e:\n",
    "        output = f\"Agent call failed, error: {e}\"\n",
    "\n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"steps\": state.get(\"steps\", []),\n",
    "        \"final\": output\n",
    "    }\n",
    "\n",
    "# ======== 7. Control flow function ========\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    \"\"\"Decides whether to continue or end the graph execution\"\"\"\n",
    "    return END if state[\"final\"] is not None else \"agent\"\n",
    "\n",
    "# ======== 8. Build LangGraph graph ========\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_agent)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue, {END: END})\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "# ======== 9. Run function ========\n",
    "def run_langgraph_agent(query: str):\n",
    "    inputs = {\"input\": query, \"steps\": [], \"final\": None}\n",
    "    result = runnable.invoke(inputs)\n",
    "    return result[\"final\"]\n",
    "\n",
    "# ======== 10. CLI interaction ========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"LangGraph RAG Agent started. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"Enter your question:\\n> \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        answer = run_langgraph_agent(query)\n",
    "        print(f\"\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a54bc7-9f1d-4e92-b432-138397da9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from typing import TypedDict, List, Union\n",
    "\n",
    "# ======== 1. Load embeddings model and vector database ========\n",
    "print(\"[INFO] Loading embeddings model and vector database...\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"qa_index_cleaned\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "print(\"[DONE] Vector database loaded\")\n",
    "\n",
    "# ======== 2. Load local Qwen2-1.5B quantized model (4bit) ========\n",
    "print(\"[INFO] Loading Qwen2-1.5B model (4bit)...\")\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "print(\"[DONE] Model loaded and ready for text generation\")\n",
    "\n",
    "# ======== 3. Build RAG QA tool ========\n",
    "print(\"[INFO] Building RAG QA chain...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "rag_tool = Tool(\n",
    "    name=\"rag_qa_tool\",\n",
    "    func=qa_chain.run,\n",
    "    description=\"Use this tool only to answer questions based on the internal knowledge base. It does NOT have any functions like 'learn' or others.\"\n",
    ")\n",
    "\n",
    "tools = [rag_tool]\n",
    "\n",
    "# ======== 4. Initialize LangChain Agent ========\n",
    "print(\"[INFO] Initializing LangChain Agent...\")\n",
    "agent_chain = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method=\"force\"  # Low iterations to avoid infinite loops\n",
    ")\n",
    "\n",
    "# ======== 5. Define Agent state structure ========\n",
    "class AgentState(TypedDict):\n",
    "    input: str\n",
    "    steps: List[tuple]\n",
    "    final: Union[str, None]\n",
    "\n",
    "# ======== 6. Agent call function ========\n",
    "def call_agent(state: AgentState) -> AgentState:\n",
    "    input_text = state[\"input\"]\n",
    "    print(f\"\\n[INFO] Calling agent_chain with question: {input_text}\")\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Check retriever\n",
    "        print(\"[INFO] Attempting document retrieval...\")\n",
    "        docs = retriever.get_relevant_documents(input_text)\n",
    "        print(f\"[INFO] Retrieved {len(docs)} documents\")\n",
    "\n",
    "        # Agent execution\n",
    "        print(\"[INFO] Calling LLM + Tool reasoning...\")\n",
    "        output = agent_chain.run(input_text)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"[DONE] agent_chain finished in {elapsed:.2f} seconds\")\n",
    "\n",
    "    except Exception as e:\n",
    "        output = f\"Agent call failed, error: {e}\"\n",
    "        print(f\"[ERROR] Execution failed: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"input\": input_text,\n",
    "        \"steps\": state.get(\"steps\", []),\n",
    "        \"final\": output\n",
    "    }\n",
    "\n",
    "# ======== 7. Control flow function ========\n",
    "def should_continue(state: AgentState) -> str:\n",
    "    return END if state[\"final\"] is not None else \"agent\"\n",
    "\n",
    "# ======== 8. Build LangGraph graph ========\n",
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"agent\", call_agent)\n",
    "graph.set_entry_point(\"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_continue, {END: END})\n",
    "\n",
    "runnable = graph.compile()\n",
    "\n",
    "# ======== 9. Run function ========\n",
    "def run_langgraph_agent(query: str):\n",
    "    print(f\"\\n[INFO] Received question: {query}\")\n",
    "    inputs = {\"input\": query, \"steps\": [], \"final\": None}\n",
    "    result = runnable.invoke(inputs)\n",
    "    print(\"[DONE] LangGraph execution completed\")\n",
    "    return result[\"final\"]\n",
    "\n",
    "# ======== 10. CLI interaction ========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"LangGraph RAG Agent started. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"Enter your question:\\n> \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        answer = run_langgraph_agent(query)\n",
    "        print(f\"\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7674aac-c065-4f05-8d9c-d32964ed4bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
