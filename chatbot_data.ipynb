{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dbd5b5-102e-430f-9ac2-f055f00f156e",
   "metadata": {},
   "source": [
    "pip install --upgrade huggingface-hub>=0.15.1 transformers>=4.30 accelerate>=0.18 sentence-transformers>=2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ef755-b713-4dfc-aea7-7e0fd111ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from glob import glob\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# 1. Load Q&A data\n",
    "data_dir = r\"C:\\Users\\15278\\CHATBOT\\data_crawl-20250703T175549Z-1-001\\data_crawl\"\n",
    "json_files = glob(os.path.join(data_dir, \"quora_all_scraped_*_extracted.json\"))\n",
    "all_texts = []\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)[\"data\"]\n",
    "        for item in data:\n",
    "            q = item.get(\"question_text\", \"\")\n",
    "            for a in item.get(\"answer_texts\", []):\n",
    "                text = f\"Q: {q}\\nA: {a}\"\n",
    "                all_texts.append(text)\n",
    "\n",
    "print(f\" Loaded Q&A pairs: {len(all_texts)}\")\n",
    "\n",
    "# 2. Use complete text directly (no chunking)\n",
    "texts = all_texts\n",
    "\n",
    "# 3. Load English Embedding model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 4. Generate embeddings & build FAISS index\n",
    "embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# 5. Save vector database and original texts\n",
    "with open(\"qa_chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(texts, f)\n",
    "\n",
    "faiss.write_index(index, \"qa_index.faiss\")\n",
    "print(\" Vector database built and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59de477-a7f1-419f-beec-ba027deb43c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load index\n",
    "index = faiss.read_index(\"qa_index.faiss\")\n",
    "\n",
    "# 2. Load text chunks (qa_chunks.pkl)\n",
    "with open(\"qa_chunks.pkl\", \"rb\") as f:\n",
    "    qa_chunks = pickle.load(f)\n",
    "\n",
    "# 3. Check if index and text chunks count match\n",
    "assert index.ntotal == len(qa_chunks), f\"Index count ({index.ntotal}) and text chunks count ({len(qa_chunks)}) do not match\"\n",
    "\n",
    "# 4. Get first 10 vectors + corresponding texts\n",
    "print(\"First 10 vectors in index and their corresponding texts:\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    vector = index.reconstruct(i)  # Get the i-th vector\n",
    "    text = qa_chunks[i]            # Get the i-th corresponding text\n",
    "    print(f\"=== Entry {i} ===\")\n",
    "    print(f\"[Vector length]: {len(vector)}\")\n",
    "    print(f\"[Text content]: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ad47a-ba41-41eb-8660-c86a2f0df72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Assuming texts and embeddings are already prepared\n",
    "# 1. Shuffle index order\n",
    "indices = list(range(len(texts)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# 2. Split by ratio\n",
    "train_ratio = 0.8\n",
    "train_size = int(len(texts) * train_ratio)\n",
    "train_indices = indices[:train_size]\n",
    "test_indices = indices[train_size:]\n",
    "\n",
    "# 3. Get train and test texts and vectors separately\n",
    "train_texts = [texts[i] for i in train_indices]\n",
    "test_texts = [texts[i] for i in test_indices]\n",
    "train_embeddings = embeddings[train_indices]\n",
    "test_embeddings = embeddings[test_indices]\n",
    "\n",
    "# 4. Save texts\n",
    "with open(\"train_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_texts, f)\n",
    "\n",
    "with open(\"test_texts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_texts, f)\n",
    "\n",
    "# 5. Build and save corresponding FAISS indices\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "train_index = faiss.IndexFlatL2(dimension)\n",
    "train_index.add(train_embeddings)\n",
    "faiss.write_index(train_index, \"train_index.faiss\")\n",
    "\n",
    "test_index = faiss.IndexFlatL2(dimension)\n",
    "test_index.add(test_embeddings)\n",
    "faiss.write_index(test_index, \"test_index.faiss\")\n",
    "\n",
    "print(f\"Training set size: {len(train_texts)}, Test set size: {len(test_texts)}\")\n",
    "print(\" Splitting completed and saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
