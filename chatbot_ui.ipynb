{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a178d-23b1-4e97-867a-084dbaf45e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import base64\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import AutoTokenizer, pipeline, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "\n",
    "# =================== 1. Base64 Image Loading ===================\n",
    "def image_to_base64(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    return \"data:image/png;base64,\" + base64.b64encode(data).decode()\n",
    "\n",
    "user_avatar = image_to_base64(\"man.png\")  # Local user avatar\n",
    "bot_avatar = image_to_base64(\"bot.png\")   # Local bot avatar\n",
    "\n",
    "# =================== 2. Load FAISS Vector Store ===================\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.load_local(\"qa_index_cleaned\", embedding_model, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "# =================== 3. Load Quantized Qwen Model ===================\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "generate_text = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)\n",
    "\n",
    "# =================== 4. Build RAG Chain ===================\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# Global chat history\n",
    "chat_history = []\n",
    "\n",
    "# =================== 5. Q&A Function ===================\n",
    "def rag_answer(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    answer = result[\"result\"]\n",
    "    source_docs = result[\"source_documents\"]\n",
    "    sources = [doc.page_content[:300] + (\"...\" if len(doc.page_content) > 300 else \"\") for doc in source_docs]\n",
    "    return answer, sources\n",
    "\n",
    "# =================== 6. Generate HTML Chat Content ===================\n",
    "def chatbot_fn(user_input):\n",
    "    global chat_history\n",
    "    answer, sources = rag_answer(user_input)\n",
    "    chat_history.append((user_input, answer, sources))\n",
    "\n",
    "    html = \"\"\n",
    "    for user_text, bot_text, sources in chat_history:\n",
    "        sources_html = \"\"\n",
    "        if sources:\n",
    "            sources_items = \"\".join([f\"<li style='margin-bottom:6px;'>{src}</li>\" for src in sources])\n",
    "            sources_html = f\"\"\"\n",
    "            <details style=\"margin-top:10px;\">\n",
    "                <summary style=\"cursor:pointer; font-weight:bold;\">View source documents ({len(sources)})</summary>\n",
    "                <ul style=\"padding-left:20px; margin-top:8px; color:#555;\">{sources_items}</ul>\n",
    "            </details>\n",
    "            \"\"\"\n",
    "        html += f\"\"\"\n",
    "        <div style=\"display:flex; align-items:flex-start; margin-bottom:10px;\">\n",
    "          <img src=\"{user_avatar}\" style=\"width:40px; height:40px; border-radius:50%; margin-right:10px;\"/>\n",
    "          <div style=\"background:#DCF8C6; padding:10px; border-radius:10px; max-width:70%; white-space:pre-wrap;\">{user_text}</div>\n",
    "        </div>\n",
    "        <div style=\"display:flex; align-items:flex-start; justify-content:flex-end; margin-bottom:20px;\">\n",
    "          <div style=\"background:#F1F0F0; padding:10px; border-radius:10px; max-width:70%; white-space:pre-wrap;\">{bot_text}{sources_html}</div>\n",
    "          <img src=\"{bot_avatar}\" style=\"width:40px; height:40px; border-radius:50%; margin-left:10px;\"/>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    return html\n",
    "\n",
    "# =================== 7. Clear Chat History ===================\n",
    "def clear_chat():\n",
    "    global chat_history\n",
    "    chat_history = []\n",
    "    return \"\"\n",
    "\n",
    "# =================== 8. Launch Gradio UI ===================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Careers Chatbot\")\n",
    "\n",
    "    chat_display = gr.HTML()\n",
    "    msg = gr.Textbox(placeholder=\"Type your question and press enter...\", label=\"Input\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "\n",
    "    msg.submit(chatbot_fn, inputs=msg, outputs=chat_display)\n",
    "    clear.click(clear_chat, outputs=chat_display)\n",
    "\n",
    "demo.launch(server_name=\"127.0.0.1\", server_port=7865)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791932f-4ac7-458f-ac4a-8623eb0f2838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
