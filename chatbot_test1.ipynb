{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ec06ce-dc79-49f4-a2f6-4772499688c6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957f047-05ef-40d2-947e-a029b530f4a0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import unsloth\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0f57cb-4c63-4132-82fa-d38e1cc30259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ğŸš¨ å¿…é¡»åœ¨å¯¼å…¥ transformers/unsloth å‰è®¾ç½®ï¼\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"D:/huggingface_cache/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b099e2f0-1a0b-456b-8a44-a1d42b432a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\4093645909.py:8: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader,TextLoader\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27fcc41-7704-4514-94c6-7012a7490ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "hf_token = \"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\"  #å»hugging faceå®˜ç½‘å…è´¹ç”³è¯·ä¸€ä¸ªtoken\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4befd37-98ee-4d5f-98cb-b445fedfccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 1. æ™ºèƒ½åˆ†å—é¢„å¤„ç†\n",
    "# ----------------------\n",
    "class SmartDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼Œä½¿ç”¨HuggingFaceçš„BAAI/bge-small-zh-v1.5æ¨¡å‹-è¿™ä¸ªæ¨¡å‹ä¸“ä¸ºRAGè€Œç”Ÿ\n",
    "        self.embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-zh-v1.5\",\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"batch_size\": 16}\n",
    "        )\n",
    "        \n",
    "    def _detect_content_type(self, text):\n",
    "        \"\"\"åŠ¨æ€å†…å®¹ç±»å‹æ£€æµ‹\"\"\"\n",
    "        # å¦‚æœæ–‡æœ¬åŒ…å«ä»£ç ç›¸å…³æ¨¡å¼ï¼ˆå¦‚defã€importã€printæˆ–ä»£ç ç¤ºä¾‹ï¼‰æ ‡è®°ä¸ºä»£ç \n",
    "        if re.search(r'def |import |print\\(|ä»£ç ç¤ºä¾‹', text):\n",
    "            return \"code\"\n",
    "        elif re.search(r'\\|.+\\|', text) and '%' in text:  # å¦‚æœæ–‡æœ¬åŒ…å«è¡¨æ ¼ç›¸å…³æ¨¡å¼ï¼ˆå¦‚|å’Œç™¾åˆ†æ¯”ï¼‰ï¼Œæ ‡è®°ä¸ºè¡¨æ ¼\n",
    "            return \"table\"\n",
    "        return \"normal\"   # å¦‚æœä¸æ»¡è¶³ä¸Šè¿°æ¡ä»¶ï¼Œæ ‡è®°ä¸ºæ™®é€šæ–‡æœ¬\n",
    "\n",
    "    def process_documents(self):\n",
    "        # åŠ è½½æ–‡æ¡£\n",
    "        # åˆ›å»ºåŠ è½½å™¨åˆ—è¡¨ï¼Œå¤„ç†çŸ¥è¯†åº“ä¸­çš„PDFå’Œæ–‡æœ¬æ–‡ä»¶\n",
    "        loaders = [\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.txt\", loader_cls=TextLoader,loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "        ]\n",
    "        # åˆå§‹åŒ–ç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨åŠ è½½çš„æ‰€æœ‰æ–‡æ¡£\n",
    "        documents = []\n",
    "        # éå†æ¯ä¸ªåŠ è½½å™¨ï¼ŒåŠ è½½æ–‡æ¡£å¹¶æ·»åŠ åˆ°documentsåˆ—è¡¨\n",
    "        for loader in loaders:\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "        # åˆ›å»ºè¯­ä¹‰åˆ†å—å™¨ï¼Œä½¿ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰åˆ†å—\n",
    "        chunker = SemanticChunker(\n",
    "            embeddings=self.embed_model, #ä½¿ç”¨æˆ‘ä»¬çš„åµŒå…¥æ¨¡å‹\n",
    "            breakpoint_threshold_amount=82,  # è®¾ç½®æ–­ç‚¹é˜ˆå€¼\n",
    "            add_start_index=True   # å¯ç”¨æ·»åŠ èµ·å§‹ç´¢å¼•åŠŸèƒ½\n",
    "        )\n",
    "        base_chunks = chunker.split_documents(documents)  # ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨å°†æ–‡æ¡£åˆ†å‰²ä¸ºåŸºæœ¬å—\n",
    "\n",
    "        # äºŒæ¬¡åŠ¨æ€åˆ†å—\n",
    "        # åˆå§‹åŒ–æœ€ç»ˆåˆ†å—åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨äºŒæ¬¡åˆ†å—ç»“æœ\n",
    "        final_chunks = []\n",
    "        # éå†æ¯ä¸ªåŸºæœ¬å—ï¼Œè¿›è¡ŒäºŒæ¬¡åŠ¨æ€åˆ†å—\n",
    "        for chunk in base_chunks:\n",
    "            content_type = self._detect_content_type(chunk.page_content)\n",
    "            if content_type == \"code\":\n",
    "                # å¦‚æœæ˜¯ä»£ç ï¼Œè®¾ç½®è¾ƒå°çš„å—å¤§å°å’Œé‡å ï¼Œç”¨äºä¿æŒä¸Šä¸‹æ–‡\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=256, chunk_overlap=64)\n",
    "            elif content_type == \"table\":\n",
    "                # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œè®¾ç½®ä¸­ç­‰å—å¤§å°å’Œé‡å \n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=384, chunk_overlap=96)\n",
    "            else:\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=512, chunk_overlap=128)\n",
    "                # å¦‚æœæ˜¯æ™®é€šæ–‡æœ¬ï¼Œè®¾ç½®è¾ƒå¤§çš„å—å¤§å°å’Œé‡å \n",
    "            final_chunks.extend(splitter.split_documents([chunk]))\n",
    "            # ä½¿ç”¨é€‚å½“çš„åˆ†å‰²å™¨å°†å—åˆ†å‰²ä¸ºæœ€ç»ˆå—ï¼Œå¹¶æ·»åŠ åˆ°åˆ—è¡¨\n",
    "        # éå†æœ€ç»ˆå—åˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_id\": f\"chunk_{i}\",\n",
    "                \"content_type\": self._detect_content_type(chunk.page_content)\n",
    "            })   # æ›´æ–°å—çš„å…ƒæ•°æ®ï¼Œæ·»åŠ å”¯ä¸€IDå’Œå†…å®¹ç±»å‹\n",
    "            \n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12453a79-c4ce-400b-863a-9a0e637b1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 2. æ··åˆæ£€ç´¢ç³»ç»Ÿ\n",
    "# ----------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, chunks):\n",
    "        # åˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œä½¿ç”¨Chromaå­˜å‚¨æ–‡æ¡£å—ï¼ŒåµŒå…¥æ¨¡å‹ä¸ºBAAI/bge-large-zh-v1.5\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embedding=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\"),\n",
    "            persist_directory=\"./vector_db\"\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºBM25æ£€ç´¢å™¨ï¼Œä»æ–‡æ¡£å—ä¸­åˆå§‹åŒ–ï¼Œåˆå§‹æ£€ç´¢æ•°é‡ä¸º5\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(\n",
    "            chunks, \n",
    "            k=5  # åˆå§‹æ£€ç´¢æ•°é‡å¤šäºæœ€ç»ˆéœ€è¦\n",
    "        )\n",
    "        \n",
    "        # åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼Œç»“åˆå‘é‡å’ŒBM25æ£€ç´¢ï¼Œæƒé‡åˆ†åˆ«ä¸º0.6å’Œ0.4\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[\n",
    "                self.vector_db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                self.bm25_retriever\n",
    "            ],\n",
    "            weights=[0.6, 0.4]  \n",
    "        )\n",
    "        \n",
    "        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹ï¼Œä½¿ç”¨BAAI/bge-reranker-large\n",
    "        self.reranker = CrossEncoder(\n",
    "            \"BAAI/bge-reranker-large\", \n",
    "            device=\"cuda\" if torch.has_cuda else \"cpu\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨æ··åˆæ£€ç´¢å™¨è·å–ç›¸å…³æ–‡æ¡£\n",
    "        docs = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # ç¬¬äºŒé˜¶æ®µï¼šä¸ºæŸ¥è¯¢å’Œæ¯ä¸ªæ–‡æ¡£åˆ›å»ºé…å¯¹ï¼Œç”¨äºé‡æ’åº\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        # ä½¿ç”¨é‡æ’åºæ¨¡å‹é¢„æµ‹é…å¯¹çš„åˆ†æ•°\n",
    "        ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # è¿”å›top_kç»“æœ\n",
    "        return [doc for doc, _ in ranked_docs[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c92fc7-fa82-4be7-9fda-5f5dba9bb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------\n",
    "# 3. RAGç³»ç»Ÿé›†æˆ\n",
    "# ----------------------\n",
    "class EnhancedRAG:\n",
    "    def __init__(self):\n",
    "        # æ–‡æ¡£å¤„ç†\n",
    "        processor = SmartDocumentProcessor()\n",
    "        chunks = processor.process_documents() #æ•´åˆæ£€ç´¢å’Œç”ŸæˆåŠŸèƒ½\n",
    "        \n",
    "        # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼Œä½¿ç”¨å¤„ç†åçš„åˆ†å—\n",
    "        self.retriever = HybridRetriever(chunks)\n",
    "        \n",
    "        # åŠ è½½å¾®è°ƒåçš„è¯­è¨€æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆå›ç­”\n",
    "        #æˆ‘ä½¿ç”¨DeepSeek-R1-Distill-Qwen-14Båœ¨çŸ¥ä¹æ¨ç†æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit = True,\n",
    "            device_map = \"auto\"\n",
    "        )\n",
    "        \n",
    "        # è®¾ç½®éšæœºç§å­\n",
    "        torch.manual_seed(666)\n",
    "        \n",
    "        # å°†æ¨¡å‹è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "    def generate_prompt(self, question, contexts):\n",
    "        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºå’Œç±»å‹ä¿¡æ¯\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[æ¥æºï¼š{doc.metadata['source']}ï¼Œç±»å‹ï¼š{doc.metadata['content_type']}]\\n{doc.page_content}\"\n",
    "            for doc in contexts\n",
    "        ])\n",
    "        # åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œè¦æ±‚åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜\n",
    "        return f\"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹å¸¦æ¥æºçš„ä¸Šä¸‹æ–‡ï¼š\n",
    "        {context_str}\n",
    "        \n",
    "        æŒ‰æ­¥éª¤æ€è€ƒå¹¶å›ç­”ï¼š{question}\n",
    "        å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯ã€‚æœ€åç”¨ä¸­æ–‡ç»™å‡ºç»“æ„åŒ–ç­”æ¡ˆã€‚\"\"\"\n",
    "\n",
    "    def ask(self, question):\n",
    "        # ä½¿ç”¨æ£€ç´¢å™¨è·å–ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡\n",
    "        contexts = self.retriever.retrieve(question)\n",
    "        \n",
    "        # æ ¹æ®é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆæç¤º\n",
    "        prompt = self.generate_prompt(question, contexts)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        # ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "        generated_ids = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            \n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        response = {'choices': [{'text': generated_text}]}\n",
    "        return response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20e74d1-b41d-4e72-9f01-da26680da12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "class EnhancedRAG:\n",
    "    def __init__(self):\n",
    "        # æ–‡æ¡£å¤„ç†ï¼ˆå‡è®¾ä½ å·²æœ‰è¿™ä¸ªç±»å’Œæ–¹æ³•ï¼‰\n",
    "        processor = SmartDocumentProcessor()\n",
    "        chunks = processor.process_documents()  # æ•´åˆæ£€ç´¢å’Œç”ŸæˆåŠŸèƒ½\n",
    "        \n",
    "        # åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼Œä½¿ç”¨å¤„ç†åçš„åˆ†å—\n",
    "        self.retriever = HybridRetriever(chunks)\n",
    "        \n",
    "        # åŠ è½½é‡åŒ–å°æ¨¡å‹ï¼Œé€‚é… 8GB æ˜¾å¡\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"unsloth/qwen2-1.5b-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # è®¾ç½®éšæœºç§å­ï¼Œä¿è¯æ¨ç†ç¨³å®šæ€§\n",
    "        torch.manual_seed(666)\n",
    "        \n",
    "        # è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼\n",
    "        self.model.eval()\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "    def generate_prompt(self, question, contexts):\n",
    "        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºå’Œç±»å‹ä¿¡æ¯\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[æ¥æºï¼š{doc.metadata['source']}ï¼Œç±»å‹ï¼š{doc.metadata['content_type']}]\\n{doc.page_content}\"\n",
    "            for doc in contexts\n",
    "        ])\n",
    "        # åˆ›å»ºæç¤ºæ¨¡æ¿ï¼Œè¦æ±‚åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜\n",
    "        return f\"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹å¸¦æ¥æºçš„ä¸Šä¸‹æ–‡ï¼š\n",
    "    {context_str}\n",
    "    \n",
    "    æŒ‰æ­¥éª¤æ€è€ƒå¹¶å›ç­”ï¼š{question}\n",
    "    å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºç¼ºå¤±çš„ä¿¡æ¯ã€‚æœ€åç”¨ä¸­æ–‡ç»™å‡ºç»“æ„åŒ–ç­”æ¡ˆã€‚\"\"\"\n",
    "    \n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            # ä½¿ç”¨æ£€ç´¢å™¨è·å–ä¸é—®é¢˜ç›¸å…³çš„ä¸Šä¸‹æ–‡\n",
    "            contexts = self.retriever.retrieve(question)\n",
    "            \n",
    "            # æ ¹æ®é—®é¢˜å’Œä¸Šä¸‹æ–‡ç”Ÿæˆæç¤º\n",
    "            prompt = self.generate_prompt(question, contexts)\n",
    "            \n",
    "            # Tokenize å¹¶è½¬åˆ° CUDA\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "       \n",
    "            \n",
    "            # ä½¿ç”¨è¯­è¨€æ¨¡å‹ç”Ÿæˆå›ç­”ï¼Œé™åˆ¶ max_new_tokens ä»¥é˜²çˆ†æ˜¾å­˜\n",
    "            generated_ids = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            # è§£ç ç”Ÿæˆæ–‡æœ¬\n",
    "            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"[æ¨¡å‹ç”Ÿæˆå¤±è´¥]: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884af6f3-9ab2-4cb6-ad3d-a823a7d9d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3047746710.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embed_model = HuggingFaceEmbeddings(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3868776009.py:31: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  device=\"cuda\" if torch.has_cuda else \"cpu\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "rag = EnhancedRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c6fcf4-85ce-42f7-b767-1361d614498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3868776009.py:36: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.ensemble_retriever.get_relevant_documents(query)\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é—®é¢˜ï¼šæˆ‘ç°åœ¨æ­£åœ¨å­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¯·ä½ è®²è®²å·ç§¯ç¥ç»ç½‘ç»œçš„åº”ç”¨\n",
      "ç­”æ¡ˆï¼š\n",
      "[æ¨¡å‹ç”Ÿæˆå¤±è´¥]: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n"
     ]
    }
   ],
   "source": [
    "complex_question = \"æˆ‘ç°åœ¨æ­£åœ¨å­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œï¼Œè¯·ä½ è®²è®²å·ç§¯ç¥ç»ç½‘ç»œçš„åº”ç”¨\"\n",
    "answer = rag.ask(complex_question)\n",
    "print(f\"é—®é¢˜ï¼š{complex_question}\")\n",
    "print(\"ç­”æ¡ˆï¼š\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07d804-489c-49a3-83d4-c9480f75657e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f029c-c460-4135-a466-30f7304581c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f8049-4c6f-4545-9f6d-0cbe9f64ebf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5b4f4-eae8-488a-a1f6-3cda8654bc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcda45e-c7c7-49c3-bcb9-17f8b429029d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
