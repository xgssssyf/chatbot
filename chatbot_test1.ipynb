{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ec06ce-dc79-49f4-a2f6-4772499688c6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957f047-05ef-40d2-947e-a029b530f4a0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import unsloth\n",
    "import torch\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    max_seq_length=4096,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0f57cb-4c63-4132-82fa-d38e1cc30259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 🚨 必须在导入 transformers/unsloth 前设置！\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/huggingface_cache/transformers\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"D:/huggingface_cache/hub\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b099e2f0-1a0b-456b-8a44-a1d42b432a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\15278\\.conda\\envs\\chatbot\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\4093645909.py:8: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.document_loaders import DirectoryLoader,PyPDFLoader,TextLoader\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f27fcc41-7704-4514-94c6-7012a7490ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "hf_token = \"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\"  #去hugging face官网免费申请一个token\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4befd37-98ee-4d5f-98cb-b445fedfccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 1. 智能分块预处理\n",
    "# ----------------------\n",
    "class SmartDocumentProcessor:\n",
    "    def __init__(self):\n",
    "        # 初始化嵌入模型，使用HuggingFace的BAAI/bge-small-zh-v1.5模型-这个模型专为RAG而生\n",
    "        self.embed_model = HuggingFaceEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-zh-v1.5\",\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"batch_size\": 16}\n",
    "        )\n",
    "        \n",
    "    def _detect_content_type(self, text):\n",
    "        \"\"\"动态内容类型检测\"\"\"\n",
    "        # 如果文本包含代码相关模式（如def、import、print或代码示例）标记为代码\n",
    "        if re.search(r'def |import |print\\(|代码示例', text):\n",
    "            return \"code\"\n",
    "        elif re.search(r'\\|.+\\|', text) and '%' in text:  # 如果文本包含表格相关模式（如|和百分比），标记为表格\n",
    "            return \"table\"\n",
    "        return \"normal\"   # 如果不满足上述条件，标记为普通文本\n",
    "\n",
    "    def process_documents(self):\n",
    "        # 加载文档\n",
    "        # 创建加载器列表，处理知识库中的PDF和文本文件\n",
    "        loaders = [\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
    "            DirectoryLoader(\"./knowledge_base\", glob=\"**/*.txt\", loader_cls=TextLoader,loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "        ]\n",
    "        # 初始化空列表，用于存储加载的所有文档\n",
    "        documents = []\n",
    "        # 遍历每个加载器，加载文档并添加到documents列表\n",
    "        for loader in loaders:\n",
    "            documents.extend(loader.load())\n",
    "\n",
    "        # 创建语义分块器，使用嵌入模型进行语义分块\n",
    "        chunker = SemanticChunker(\n",
    "            embeddings=self.embed_model, #使用我们的嵌入模型\n",
    "            breakpoint_threshold_amount=82,  # 设置断点阈值\n",
    "            add_start_index=True   # 启用添加起始索引功能\n",
    "        )\n",
    "        base_chunks = chunker.split_documents(documents)  # 使用语义分块器将文档分割为基本块\n",
    "\n",
    "        # 二次动态分块\n",
    "        # 初始化最终分块列表，用于存储二次分块结果\n",
    "        final_chunks = []\n",
    "        # 遍历每个基本块，进行二次动态分块\n",
    "        for chunk in base_chunks:\n",
    "            content_type = self._detect_content_type(chunk.page_content)\n",
    "            if content_type == \"code\":\n",
    "                # 如果是代码，设置较小的块大小和重叠，用于保持上下文\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=256, chunk_overlap=64)\n",
    "            elif content_type == \"table\":\n",
    "                # 如果是表格，设置中等块大小和重叠\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=384, chunk_overlap=96)\n",
    "            else:\n",
    "                splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=512, chunk_overlap=128)\n",
    "                # 如果是普通文本，设置较大的块大小和重叠\n",
    "            final_chunks.extend(splitter.split_documents([chunk]))\n",
    "            # 使用适当的分割器将块分割为最终块，并添加到列表\n",
    "        # 遍历最终块列表，为每个块添加元数据\n",
    "        for i, chunk in enumerate(final_chunks):\n",
    "            chunk.metadata.update({\n",
    "                \"chunk_id\": f\"chunk_{i}\",\n",
    "                \"content_type\": self._detect_content_type(chunk.page_content)\n",
    "            })   # 更新块的元数据，添加唯一ID和内容类型\n",
    "            \n",
    "        return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12453a79-c4ce-400b-863a-9a0e637b1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 2. 混合检索系统\n",
    "# ----------------------\n",
    "class HybridRetriever:\n",
    "    def __init__(self, chunks):\n",
    "        # 创建向量数据库，使用Chroma存储文档块，嵌入模型为BAAI/bge-large-zh-v1.5\n",
    "        self.vector_db = Chroma.from_documents(\n",
    "            chunks,\n",
    "            embedding=HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-zh-v1.5\"),\n",
    "            persist_directory=\"./vector_db\"\n",
    "        )\n",
    "        \n",
    "        # 创建BM25检索器，从文档块中初始化，初始检索数量为5\n",
    "        self.bm25_retriever = BM25Retriever.from_documents(\n",
    "            chunks, \n",
    "            k=5  # 初始检索数量多于最终需要\n",
    "        )\n",
    "        \n",
    "        # 创建混合检索器，结合向量和BM25检索，权重分别为0.6和0.4\n",
    "        self.ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[\n",
    "                self.vector_db.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                self.bm25_retriever\n",
    "            ],\n",
    "            weights=[0.6, 0.4]  \n",
    "        )\n",
    "        \n",
    "        # 初始化重排序模型，使用BAAI/bge-reranker-large\n",
    "        self.reranker = CrossEncoder(\n",
    "            \"BAAI/bge-reranker-large\", \n",
    "            device=\"cuda\" if torch.has_cuda else \"cpu\"\n",
    "        )\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        # 第一阶段：使用混合检索器获取相关文档\n",
    "        docs = self.ensemble_retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # 第二阶段：为查询和每个文档创建配对，用于重排序\n",
    "        pairs = [[query, doc.page_content] for doc in docs]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "        # 使用重排序模型预测配对的分数\n",
    "        ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 返回top_k结果\n",
    "        return [doc for doc, _ in ranked_docs[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c92fc7-fa82-4be7-9fda-5f5dba9bb2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ----------------------\n",
    "# 3. RAG系统集成\n",
    "# ----------------------\n",
    "class EnhancedRAG:\n",
    "    def __init__(self):\n",
    "        # 文档处理\n",
    "        processor = SmartDocumentProcessor()\n",
    "        chunks = processor.process_documents() #整合检索和生成功能\n",
    "        \n",
    "        # 初始化混合检索器，使用处理后的分块\n",
    "        self.retriever = HybridRetriever(chunks)\n",
    "        \n",
    "        # 加载微调后的语言模型，用于生成回答\n",
    "        #我使用DeepSeek-R1-Distill-Qwen-14B在知乎推理数据集上进行微调\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit = True,\n",
    "            device_map = \"auto\"\n",
    "        )\n",
    "        \n",
    "        # 设置随机种子\n",
    "        torch.manual_seed(666)\n",
    "        \n",
    "        # 将模型设置为推理模式\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "    def generate_prompt(self, question, contexts):\n",
    "        # 格式化上下文，包含来源和类型信息\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[来源：{doc.metadata['source']}，类型：{doc.metadata['content_type']}]\\n{doc.page_content}\"\n",
    "            for doc in contexts\n",
    "        ])\n",
    "        # 创建提示模板，要求基于上下文回答问题\n",
    "        return f\"\"\"你是一个专业助手，请严格根据以下带来源的上下文：\n",
    "        {context_str}\n",
    "        \n",
    "        按步骤思考并回答：{question}\n",
    "        如果上下文信息不足，请明确指出缺失的信息。最后用中文给出结构化答案。\"\"\"\n",
    "\n",
    "    def ask(self, question):\n",
    "        # 使用检索器获取与问题相关的上下文\n",
    "        contexts = self.retriever.retrieve(question)\n",
    "        \n",
    "        # 根据问题和上下文生成提示\n",
    "        prompt = self.generate_prompt(question, contexts)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        # 使用语言模型生成回答\n",
    "        generated_ids = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            \n",
    "            do_sample=True\n",
    "        )\n",
    "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        response = {'choices': [{'text': generated_text}]}\n",
    "        return response['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20e74d1-b41d-4e72-9f01-da26680da12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "class EnhancedRAG:\n",
    "    def __init__(self):\n",
    "        # 文档处理（假设你已有这个类和方法）\n",
    "        processor = SmartDocumentProcessor()\n",
    "        chunks = processor.process_documents()  # 整合检索和生成功能\n",
    "        \n",
    "        # 初始化混合检索器，使用处理后的分块\n",
    "        self.retriever = HybridRetriever(chunks)\n",
    "        \n",
    "        # 加载量化小模型，适配 8GB 显卡\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            \"unsloth/qwen2-1.5b-bnb-4bit\",\n",
    "            max_seq_length=2048,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        # 设置随机种子，保证推理稳定性\n",
    "        torch.manual_seed(666)\n",
    "        \n",
    "        # 设置模型为推理模式\n",
    "        self.model.eval()\n",
    "        FastLanguageModel.for_inference(self.model)\n",
    "        \n",
    "    def generate_prompt(self, question, contexts):\n",
    "        # 格式化上下文，包含来源和类型信息\n",
    "        context_str = \"\\n\\n\".join([\n",
    "            f\"[来源：{doc.metadata['source']}，类型：{doc.metadata['content_type']}]\\n{doc.page_content}\"\n",
    "            for doc in contexts\n",
    "        ])\n",
    "        # 创建提示模板，要求基于上下文回答问题\n",
    "        return f\"\"\"你是一个专业助手，请严格根据以下带来源的上下文：\n",
    "    {context_str}\n",
    "    \n",
    "    按步骤思考并回答：{question}\n",
    "    如果上下文信息不足，请明确指出缺失的信息。最后用中文给出结构化答案。\"\"\"\n",
    "    \n",
    "    def ask(self, question):\n",
    "        try:\n",
    "            # 使用检索器获取与问题相关的上下文\n",
    "            contexts = self.retriever.retrieve(question)\n",
    "            \n",
    "            # 根据问题和上下文生成提示\n",
    "            prompt = self.generate_prompt(question, contexts)\n",
    "            \n",
    "            # Tokenize 并转到 CUDA\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "       \n",
    "            \n",
    "            # 使用语言模型生成回答，限制 max_new_tokens 以防爆显存\n",
    "            generated_ids = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.3,\n",
    "                top_p=0.9,\n",
    "                do_sample=True\n",
    "            )\n",
    "            \n",
    "            # 解码生成文本\n",
    "            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            return generated_text\n",
    "        \n",
    "        except Exception as e:\n",
    "            return f\"[模型生成失败]: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884af6f3-9ab2-4cb6-ad3d-a823a7d9d833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3047746710.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embed_model = HuggingFaceEmbeddings(\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3868776009.py:31: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  device=\"cuda\" if torch.has_cuda else \"cpu\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.3: Fast Qwen2 patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Laptop GPU. Num GPUs = 1. Max memory: 7.996 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "rag = EnhancedRAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11c6fcf4-85ce-42f7-b767-1361d614498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15278\\AppData\\Local\\Temp\\ipykernel_60876\\3868776009.py:36: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = self.ensemble_retriever.get_relevant_documents(query)\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问题：我现在正在学习卷积神经网络，请你讲讲卷积神经网络的应用\n",
      "答案：\n",
      "[模型生成失败]: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous\n"
     ]
    }
   ],
   "source": [
    "complex_question = \"我现在正在学习卷积神经网络，请你讲讲卷积神经网络的应用\"\n",
    "answer = rag.ask(complex_question)\n",
    "print(f\"问题：{complex_question}\")\n",
    "print(\"答案：\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07d804-489c-49a3-83d4-c9480f75657e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79f029c-c460-4135-a466-30f7304581c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f8049-4c6f-4545-9f6d-0cbe9f64ebf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c5b4f4-eae8-488a-a1f6-3cda8654bc02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcda45e-c7c7-49c3-bcb9-17f8b429029d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
