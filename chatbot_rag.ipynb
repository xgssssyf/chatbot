{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b227ba-357e-465d-be91-d3b0db35b82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# ===== 1. Load test Q/A texts =====\n",
    "with open(\"test_texts.pkl\", \"rb\") as f:\n",
    "    test_texts = pickle.load(f)\n",
    "\n",
    "# ===== 2. Load retrieval data =====\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "index = faiss.read_index(\"qa_index_cleaned.faiss\")\n",
    "with open(\"qa_chunks_cleaned.pkl\", \"rb\") as f:\n",
    "    texts = pickle.load(f)\n",
    "\n",
    "# ===== 3. Load local Qwen model =====\n",
    "model_name = \"unsloth/qwen2-1.5b-bnb-4bit\"\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model.eval()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===== 4. Helper function: extract question and answer =====\n",
    "def extract_qa(text):\n",
    "    q_match = re.search(r\"Q:\\s*(.*)\", text)\n",
    "    a_match = re.search(r\"A:\\s*(.*)\", text, re.DOTALL)\n",
    "    question = q_match.group(1).strip() if q_match else \"\"\n",
    "    answer = a_match.group(1).strip() if a_match else \"\"\n",
    "    return question, answer\n",
    "\n",
    "# ===== 5. Retrieve contexts =====\n",
    "def retrieve_contexts(query, top_k=3):\n",
    "    query_vec = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return [texts[i] for i in I[0]]\n",
    "\n",
    "# ===== 6. Generate answer with Qwen =====\n",
    "def answer_question_with_context(query, context):\n",
    "    prompt = f\"\"\"You are an intelligent QA assistant. Please answer the user's question based on the following background knowledge:\n",
    "\n",
    "Background documents:\n",
    "{context}\n",
    "\n",
    "User question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated[len(prompt):].strip()\n",
    "\n",
    "# ===== 7. Build dataset for local evaluation =====\n",
    "def build_ragas_dataset(test_texts, max_samples=100):\n",
    "    questions, answers, contexts, references = [], [], [], []\n",
    "    for i, item in enumerate(test_texts[:max_samples]):\n",
    "        q, a = extract_qa(item)\n",
    "        if not q or not a:\n",
    "            print(f\"Skipping sample {i+1}: missing question or answer\")\n",
    "            continue\n",
    "        ctxs = retrieve_contexts(q)\n",
    "        context = \"\\n---\\n\".join(ctxs)\n",
    "        pred = answer_question_with_context(q, context)\n",
    "        questions.append(q)\n",
    "        answers.append(pred)\n",
    "        contexts.append(ctxs)\n",
    "        references.append(a)\n",
    "        print(f\" Sample {i+1}/{max_samples} completed\")\n",
    "    dataset_dict = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": references\n",
    "    }\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# ===== 8. Local ragas-lite evaluation functions =====\n",
    "def average_cos_sim(a, b):\n",
    "    return cosine_similarity(a, b).mean()\n",
    "\n",
    "def evaluate_ragas_locally(dataset, embedder):\n",
    "    print(\"\\n Running local embedding-based ragas-lite evaluation...\\n\")\n",
    "    scores = {\n",
    "        \"faithfulness\": [],\n",
    "        \"answer_relevancy\": [],\n",
    "        \"context_precision\": [],\n",
    "        \"context_recall\": [],\n",
    "    }\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        q = dataset[\"question\"][i]\n",
    "        a = dataset[\"answer\"][i]\n",
    "        gt = dataset[\"ground_truth\"][i]\n",
    "        ctxs = dataset[\"contexts\"][i]\n",
    "\n",
    "        try:\n",
    "            q_vec = embedder.encode([q])\n",
    "            a_vec = embedder.encode([a])\n",
    "            gt_vec = embedder.encode([gt])\n",
    "            ctx_vecs = embedder.encode(ctxs)\n",
    "\n",
    "            # 1. Faithfulness: answer vs. context similarity\n",
    "            faith = average_cos_sim(a_vec, ctx_vecs)\n",
    "            scores[\"faithfulness\"].append(faith)\n",
    "\n",
    "            # 2. Answer relevancy: answer vs. question similarity\n",
    "            rel = cosine_similarity(a_vec, q_vec)[0][0]\n",
    "            scores[\"answer_relevancy\"].append(rel)\n",
    "\n",
    "            # 3. Context precision: ground truth vs. most relevant context\n",
    "            precision = max([cosine_similarity(gt_vec, c.reshape(1, -1))[0][0] for c in ctx_vecs])\n",
    "            scores[\"context_precision\"].append(precision)\n",
    "\n",
    "            # 4. Context recall: ground truth vs. all contexts (average)\n",
    "            recall = average_cos_sim(gt_vec, ctx_vecs)\n",
    "            scores[\"context_recall\"].append(recall)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" Evaluation failed at sample {i+1}: {e}\")\n",
    "\n",
    "    # Print average scores\n",
    "    print(\"\\n Local ragas-lite evaluation completed:\\n\")\n",
    "    for k, v in scores.items():\n",
    "        avg = np.mean(v)\n",
    "        print(f\"{k}: {avg:.3f}\")\n",
    "\n",
    "# ===== 9. Main entry point =====\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path = \"ragas_data.pkl\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\" Found saved dataset, loading {dataset_path} ...\")\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            ragas_data = pickle.load(f)\n",
    "    else:\n",
    "        print(\" Building dataset...\")\n",
    "        ragas_data = build_ragas_dataset(test_texts, max_samples=100)\n",
    "        print(f\" Saving dataset to {dataset_path} ...\")\n",
    "        with open(dataset_path, \"wb\") as f:\n",
    "            pickle.dump(ragas_data, f)\n",
    "\n",
    "    print(\"\\n Dataset ready, starting evaluation...\\n\")\n",
    "    evaluate_ragas_locally(ragas_data, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf10be8c-6bdc-4c7c-bc6d-9dae3af7809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import jieba\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# ===== 1. Document Preprocessing Layer =====\n",
    "class DocumentPreprocessor:\n",
    "    def __init__(self, chunk_size=512, chunk_overlap=50):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\", \"，\", \" \", \"\"],\n",
    "            keep_separator=True,\n",
    "            length_function=len\n",
    "        )\n",
    "    \n",
    "    def dynamic_chunking(self, documents):\n",
    "        print(\"Starting dynamic chunking...\")\n",
    "        all_chunks = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            chunks = self.text_splitter.split_text(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "        print(f\"Dynamic chunking completed. Generated {len(all_chunks)} chunks.\")\n",
    "        return all_chunks\n",
    "\n",
    "# ===== 2. Hybrid Retrieval Layer =====\n",
    "class HybridRetriever:\n",
    "    def __init__(self, embedder_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(embedder_model)\n",
    "        self.faiss_index = None\n",
    "        self.texts = None\n",
    "        self.bm25 = None\n",
    "        self.tokenized_docs = None\n",
    "    \n",
    "    def build_vector_index(self, texts):\n",
    "        print(\"Building vector index...\")\n",
    "        self.texts = texts\n",
    "        embeddings = self.embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(dimension)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.faiss_index.add(embeddings.astype('float32'))\n",
    "        print(f\"Vector index built. Dimension: {dimension}, Documents: {len(texts)}\")\n",
    "    \n",
    "    def build_bm25_index(self, texts):\n",
    "        print(\"Building BM25 keyword index...\")\n",
    "        self.tokenized_docs = [list(jieba.cut(text)) for text in texts]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        print(\"BM25 index built.\")\n",
    "    \n",
    "    def vector_search(self, query, top_k=5):\n",
    "        query_vec = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.faiss_index.search(query_vec.astype('float32'), top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append({'text': self.texts[idx], 'score': float(score), 'method': 'vector'})\n",
    "        return results\n",
    "    \n",
    "    def bm25_search(self, query, top_k=5):\n",
    "        query_tokens = list(jieba.cut(query))\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({'text': self.texts[idx], 'score': float(scores[idx]), 'method': 'bm25'})\n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=6, vector_weight=0.6, bm25_weight=0.4):\n",
    "        vector_results = self.vector_search(query, top_k)\n",
    "        bm25_results = self.bm25_search(query, top_k)\n",
    "        combined_results = {}\n",
    "        for result in vector_results:\n",
    "            text = result['text']\n",
    "            if text not in combined_results:\n",
    "                combined_results[text] = {'text': text, 'vector_score': 0, 'bm25_score': 0}\n",
    "            combined_results[text]['vector_score'] = result['score']\n",
    "        for result in bm25_results:\n",
    "            text = result['text']\n",
    "            if text not in combined_results:\n",
    "                combined_results[text] = {'text': text, 'vector_score': 0, 'bm25_score': 0}\n",
    "            combined_results[text]['bm25_score'] = result['score']\n",
    "        final_results = []\n",
    "        for item in combined_results.values():\n",
    "            hybrid_score = (vector_weight * item['vector_score'] + bm25_weight * item['bm25_score'])\n",
    "            final_results.append({'text': item['text'], 'score': hybrid_score, 'method': 'hybrid'})\n",
    "        final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return final_results[:top_k]\n",
    "\n",
    "# ===== 3. Query Enhancement Layer (HyDE) =====\n",
    "class QueryEnhancer:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def generate_hypothetical_document(self, query):\n",
    "        hyde_prompt = f\"\"\"Please generate a detailed hypothetical document based on the following question. The document should include the answer and related background information:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Hypothetical Document:\"\"\"\n",
    "        inputs = self.tokenizer(hyde_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        hypothetical_doc = generated[len(hyde_prompt):].strip()\n",
    "        return hypothetical_doc\n",
    "    \n",
    "    def enhanced_query(self, original_query):\n",
    "        hypothetical_doc = self.generate_hypothetical_document(original_query)\n",
    "        enhanced_query = f\"{original_query} {hypothetical_doc}\"\n",
    "        return enhanced_query, hypothetical_doc\n",
    "\n",
    "# ===== 4. Answer Generation Layer =====\n",
    "class QAChain:\n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def format_context(self, retrieved_docs):\n",
    "        if not retrieved_docs:\n",
    "            return \"No relevant background information available.\"\n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            context_parts.append(f\"Document {i}: {doc['text']}\")\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query, context):\n",
    "        qa_prompt = f\"\"\"You are an intelligent QA assistant. Please answer the user's question based on the provided background knowledge.\n",
    "\n",
    "Background knowledge:\n",
    "{context}\n",
    "\n",
    "User question: {query}\n",
    "\n",
    "Please provide an accurate and detailed answer:\"\"\"\n",
    "        inputs = self.tokenizer(qa_prompt, return_tensors=\"pt\", truncation=True, max_length=1800).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated[len(qa_prompt):].strip()\n",
    "        return answer\n",
    "\n",
    "# ===== 5. Complete RAG System =====\n",
    "class CompleteRAGSystem:\n",
    "    def __init__(self, model_name=\"unsloth/qwen2-1.5b-bnb-4bit\"):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.retriever = HybridRetriever()\n",
    "        print(\"Loading generation model...\")\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.query_enhancer = QueryEnhancer(self.model, self.tokenizer, self.device)\n",
    "        self.qa_chain = QAChain(self.model, self.tokenizer, self.device)\n",
    "    \n",
    "    def build_knowledge_base(self, documents):\n",
    "        chunks = self.preprocessor.dynamic_chunking(documents)\n",
    "        self.retriever.build_vector_index(chunks)\n",
    "        self.retriever.build_bm25_index(chunks)\n",
    "        return chunks\n",
    "    \n",
    "    def answer_question(self, query, use_hyde=True, use_hybrid=True):\n",
    "        if use_hyde:\n",
    "            enhanced_query, hypothetical_doc = self.query_enhancer.enhanced_query(query)\n",
    "            search_query = enhanced_query\n",
    "        else:\n",
    "            search_query = query\n",
    "        if use_hybrid:\n",
    "            retrieved_docs = self.retriever.hybrid_search(search_query, top_k=3)\n",
    "        else:\n",
    "            retrieved_docs = self.retriever.vector_search(search_query, top_k=3)\n",
    "        context = self.qa_chain.format_context(retrieved_docs)\n",
    "        answer = self.qa_chain.generate_answer(query, context)\n",
    "        return {'question': query, 'answer': answer, 'retrieved_docs': retrieved_docs, 'context': context}\n",
    "\n",
    "# ===== 6. Main Interactive QA Entry =====\n",
    "def main():\n",
    "    # Load document library\n",
    "    with open(\"test_texts.pkl\", \"rb\") as f:\n",
    "        test_texts = pickle.load(f)\n",
    "    documents = [text for text in test_texts]\n",
    "    \n",
    "    # Initialize system and build knowledge base\n",
    "    rag_system = CompleteRAGSystem()\n",
    "    rag_system.build_knowledge_base(documents)\n",
    "    \n",
    "    # Command-line interaction\n",
    "    print(\"\\nRAG QA System Started! (Enter q to exit)\")\n",
    "    while True:\n",
    "        user_q = input(\"\\nEnter your question: \")\n",
    "        if user_q.strip().lower() == \"q\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        result = rag_system.answer_question(user_q, use_hyde=True, use_hybrid=True)\n",
    "        print(\"\\nAnswer:\", result['answer'])\n",
    "        print(\"\\nRelevant Documents:\")\n",
    "        for i, doc in enumerate(result['retrieved_docs'], 1):\n",
    "            print(f\"--- Document {i} (score={doc['score']:.4f}, method={doc['method']}) ---\")\n",
    "            print(doc['text'][:200], \"...\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa58ea-8974-4072-8222-71f0c08058f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from rank_bm25 import BM25Okapi\n",
    "import jieba\n",
    "import os\n",
    "\n",
    "# ===== 1. Document Preprocessing Layer =====\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Document preprocessing layer: dynamic chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size=512, chunk_overlap=50):\n",
    "        # Use RecursiveCharacterTextSplitter for dynamic chunking\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \"；\", \"，\", \" \", \"\"],\n",
    "            keep_separator=True,\n",
    "            length_function=len\n",
    "        )\n",
    "    \n",
    "    def dynamic_chunking(self, documents):\n",
    "        \"\"\"Perform dynamic chunking based on semantic structure\"\"\"\n",
    "        print(\"Starting dynamic chunking...\")\n",
    "        all_chunks = []\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            chunks = self.text_splitter.split_text(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"Processed {i + 1} documents\")\n",
    "        \n",
    "        print(f\"Dynamic chunking completed. Generated {len(all_chunks)} chunks\")\n",
    "        return all_chunks\n",
    "\n",
    "# ===== 2. Embedding and Indexing Layer =====\n",
    "class HybridRetriever:\n",
    "    \"\"\"Hybrid retriever layer: combines vector and keyword search\"\"\"\n",
    "    \n",
    "    def __init__(self, embedder_model=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.embedder = SentenceTransformer(embedder_model)\n",
    "        self.faiss_index = None\n",
    "        self.texts = None\n",
    "        self.bm25 = None\n",
    "        self.tokenized_docs = None\n",
    "    \n",
    "    def build_vector_index(self, texts):\n",
    "        \"\"\"Build FAISS vector index\"\"\"\n",
    "        print(\"Building vector index...\")\n",
    "        self.texts = texts\n",
    "        \n",
    "        embeddings = self.embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.faiss_index = faiss.IndexFlatIP(dimension)\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.faiss_index.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"Vector index built. Dimension: {dimension}, Documents: {len(texts)}\")\n",
    "    \n",
    "    def build_bm25_index(self, texts):\n",
    "        \"\"\"Build BM25 keyword index\"\"\"\n",
    "        print(\"Building BM25 keyword index...\")\n",
    "        self.tokenized_docs = [list(jieba.cut(text)) for text in texts]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        print(\"BM25 index built\")\n",
    "    \n",
    "    def vector_search(self, query, top_k=5):\n",
    "        \"\"\"Vector search\"\"\"\n",
    "        if self.faiss_index is None:\n",
    "            raise ValueError(\"Vector index not built. Call build_vector_index first.\")\n",
    "        \n",
    "        query_vec = self.embedder.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(query_vec)\n",
    "        scores, indices = self.faiss_index.search(query_vec.astype('float32'), top_k)\n",
    "        \n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx != -1:\n",
    "                results.append({'text': self.texts[idx], 'score': float(score), 'method': 'vector'})\n",
    "        return results\n",
    "    \n",
    "    def bm25_search(self, query, top_k=5):\n",
    "        \"\"\"BM25 keyword search\"\"\"\n",
    "        if self.bm25 is None:\n",
    "            raise ValueError(\"BM25 index not built. Call build_bm25_index first.\")\n",
    "        \n",
    "        query_tokens = list(jieba.cut(query))\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({'text': self.texts[idx], 'score': float(scores[idx]), 'method': 'bm25'})\n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(self, query, top_k=6, vector_weight=0.6, bm25_weight=0.4):\n",
    "        \"\"\"Hybrid search: combine vector and BM25 search\"\"\"\n",
    "        vector_results = self.vector_search(query, top_k)\n",
    "        bm25_results = self.bm25_search(query, top_k)\n",
    "        combined_results = {}\n",
    "        \n",
    "        for result in vector_results:\n",
    "            text = result['text']\n",
    "            if text not in combined_results:\n",
    "                combined_results[text] = {'text': text, 'vector_score': 0, 'bm25_score': 0}\n",
    "            combined_results[text]['vector_score'] = result['score']\n",
    "        \n",
    "        for result in bm25_results:\n",
    "            text = result['text']\n",
    "            if text not in combined_results:\n",
    "                combined_results[text] = {'text': text, 'vector_score': 0, 'bm25_score': 0}\n",
    "            combined_results[text]['bm25_score'] = result['score']\n",
    "        \n",
    "        final_results = []\n",
    "        for item in combined_results.values():\n",
    "            hybrid_score = (vector_weight * item['vector_score'] + bm25_weight * item['bm25_score'])\n",
    "            final_results.append({'text': item['text'], 'score': hybrid_score, 'method': 'hybrid'})\n",
    "        \n",
    "        final_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return final_results[:top_k]\n",
    "\n",
    "# ===== 3. Query Enhancement Layer =====\n",
    "class QueryEnhancer:\n",
    "    \"\"\"Query enhancement using HyDE (hypothetical document generation)\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def generate_hypothetical_document(self, query):\n",
    "        prompt = f\"\"\"Generate a detailed hypothetical document for the following question. Include the answer and related background information:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Hypothetical Document:\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        hypothetical_doc = generated[len(prompt):].strip()\n",
    "        return hypothetical_doc\n",
    "    \n",
    "    def enhanced_query(self, original_query):\n",
    "        hypothetical_doc = self.generate_hypothetical_document(original_query)\n",
    "        enhanced_query = f\"{original_query} {hypothetical_doc}\"\n",
    "        return enhanced_query, hypothetical_doc\n",
    "\n",
    "# ===== 4. QA Generation Layer =====\n",
    "class QAChain:\n",
    "    \"\"\"QA Chain for structured question answering\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, device):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "    \n",
    "    def format_context(self, retrieved_docs):\n",
    "        if not retrieved_docs:\n",
    "            return \"No relevant background information available.\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            if isinstance(doc, dict):\n",
    "                context_parts.append(f\"Document {i}: {doc['text']}\")\n",
    "            else:\n",
    "                context_parts.append(f\"Document {i}: {doc}\")\n",
    "        return \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    def generate_answer(self, query, context):\n",
    "        prompt = f\"\"\"You are an intelligent QA assistant. Answer the user's question based on the following background knowledge.\n",
    "\n",
    "Background:\n",
    "{context}\n",
    "\n",
    "User question: {query}\n",
    "\n",
    "Provide an accurate and detailed answer:\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1800).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated[len(prompt):].strip()\n",
    "        return answer\n",
    "\n",
    "# ===== 5. Complete RAG System =====\n",
    "class CompleteRAGSystem:\n",
    "    \"\"\"Full RAG system integrating all layers\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"unsloth/qwen2-1.5b-bnb-4bit\"):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.retriever = HybridRetriever()\n",
    "        print(\"Loading generation model...\")\n",
    "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=model_name,\n",
    "            max_seq_length=2048,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.query_enhancer = QueryEnhancer(self.model, self.tokenizer, self.device)\n",
    "        self.qa_chain = QAChain(self.model, self.tokenizer, self.device)\n",
    "    \n",
    "    def build_knowledge_base(self, documents):\n",
    "        chunks = self.preprocessor.dynamic_chunking(documents)\n",
    "        self.retriever.build_vector_index(chunks)\n",
    "        self.retriever.build_bm25_index(chunks)\n",
    "        return chunks\n",
    "    \n",
    "    def retrieve_contexts(self, query, top_k=3, use_hyde=True, use_hybrid=True):\n",
    "        if use_hyde:\n",
    "            enhanced_query, _ = self.query_enhancer.enhanced_query(query)\n",
    "            search_query = enhanced_query\n",
    "        else:\n",
    "            search_query = query\n",
    "        \n",
    "        if use_hybrid:\n",
    "            retrieved_docs = self.retriever.hybrid_search(search_query, top_k=top_k)\n",
    "        else:\n",
    "            retrieved_docs = self.retriever.vector_search(search_query, top_k=top_k)\n",
    "        \n",
    "        return [doc['text'] for doc in retrieved_docs]\n",
    "    \n",
    "    def answer_question_with_context(self, query, context):\n",
    "        if isinstance(context, list):\n",
    "            context = \"\\n---\\n\".join(context)\n",
    "        return self.qa_chain.generate_answer(query, context)\n",
    "    \n",
    "    def answer_question(self, query, use_hyde=True, use_hybrid=True):\n",
    "        print(f\"\\nUser question: {query}\")\n",
    "        retrieved_contexts = self.retrieve_contexts(query, top_k=3, use_hyde=use_hyde, use_hybrid=use_hybrid)\n",
    "        context = \"\\n---\\n\".join(retrieved_contexts)\n",
    "        answer = self.answer_question_with_context(query, context)\n",
    "        return {'question': query, 'answer': answer, 'contexts': retrieved_contexts, 'context': context}\n",
    "\n",
    "# ===== 6. Helper function: extract QA =====\n",
    "def extract_qa(text):\n",
    "    q_match = re.search(r\"Q:\\s*(.*)\", text)\n",
    "    a_match = re.search(r\"A:\\s*(.*)\", text, re.DOTALL)\n",
    "    question = q_match.group(1).strip() if q_match else \"\"\n",
    "    answer = a_match.group(1).strip() if a_match else \"\"\n",
    "    return question, answer\n",
    "\n",
    "# ===== 7. Build dataset for local evaluation =====\n",
    "def build_ragas_dataset(test_texts, rag_system, max_samples=100, use_hyde=True, use_hybrid=True):\n",
    "    questions, answers, contexts, references = [], [], [], []\n",
    "    for i, item in enumerate(test_texts[:max_samples]):\n",
    "        q, a = extract_qa(item)\n",
    "        if not q or not a:\n",
    "            print(f\"Skipping sample {i+1}: missing question or answer\")\n",
    "            continue\n",
    "        ctxs = rag_system.retrieve_contexts(q, top_k=3, use_hyde=use_hyde, use_hybrid=use_hybrid)\n",
    "        context = \"\\n---\\n\".join(ctxs)\n",
    "        pred = rag_system.answer_question_with_context(q, context)\n",
    "        questions.append(q)\n",
    "        answers.append(pred)\n",
    "        contexts.append(ctxs)\n",
    "        references.append(a)\n",
    "        print(f\"Sample {i+1}/{max_samples} processed\")\n",
    "    dataset_dict = {\"question\": questions, \"answer\": answers, \"contexts\": contexts, \"ground_truth\": references}\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# ===== 8. Local ragas-lite evaluation =====\n",
    "def average_cos_sim(a, b):\n",
    "    return cosine_similarity(a, b).mean()\n",
    "\n",
    "def evaluate_ragas_locally(dataset, embedder):\n",
    "    print(\"Evaluating ragas-lite metrics using local embeddings...\\n\")\n",
    "    scores = {\"faithfulness\": [], \"answer_relevancy\": [], \"context_precision\": [], \"context_recall\": []}\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        q = dataset[\"question\"][i]\n",
    "        a = dataset[\"answer\"][i]\n",
    "        gt = dataset[\"ground_truth\"][i]\n",
    "        ctxs = dataset[\"contexts\"][i]\n",
    "        try:\n",
    "            q_vec = embedder.encode([q])\n",
    "            a_vec = embedder.encode([a])\n",
    "            gt_vec = embedder.encode([gt])\n",
    "            ctx_vecs = embedder.encode(ctxs)\n",
    "            scores[\"faithfulness\"].append(average_cos_sim(a_vec, ctx_vecs))\n",
    "            scores[\"answer_relevancy\"].append(cosine_similarity(a_vec, q_vec)[0][0])\n",
    "            scores[\"context_precision\"].append(max([cosine_similarity(gt_vec, c.reshape(1,-1))[0][0] for c in ctx_vecs]))\n",
    "            scores[\"context_recall\"].append(average_cos_sim(gt_vec, ctx_vecs))\n",
    "        except Exception as e:\n",
    "            print(f\"Evaluation failed for sample {i+1}: {e}\")\n",
    "    \n",
    "    print(\"Local ragas-lite evaluation completed:\\n\")\n",
    "    for k, v in scores.items():\n",
    "        avg = np.mean(v)\n",
    "        print(f\"{k}: {avg:.3f}\")\n",
    "    return scores\n",
    "\n",
    "# ===== 9. Main entry point =====\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"test_texts.pkl\", \"rb\") as f:\n",
    "        test_texts = pickle.load(f)\n",
    "    \n",
    "    print(\"Initializing RAG system...\")\n",
    "    rag_system = CompleteRAGSystem()\n",
    "    \n",
    "    print(\"Building knowledge base...\")\n",
    "    chunks = rag_system.build_knowledge_base(test_texts)\n",
    "    \n",
    "    dataset_path = \"enhanced_ragas_data.pkl\"\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Found saved dataset, loading {dataset_path}...\")\n",
    "        with open(dataset_path, \"rb\") as f:\n",
    "            ragas_data = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Building enhanced RAG evaluation dataset...\")\n",
    "        ragas_data = build_ragas_dataset(test_texts, rag_system, max_samples=50, use_hyde=True, use_hybrid=True)\n",
    "        print(f\"Saving dataset to {dataset_path}...\")\n",
    "        with open(dataset_path, \"wb\") as f:\n",
    "            pickle.dump(ragas_data, f)\n",
    "    \n",
    "    print(\"Dataset ready, starting evaluation...\\n\")\n",
    "    embedder = rag_system.retriever.embedder\n",
    "    scores = evaluate_ragas_locally(ragas_data, embedder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chatbot)",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
